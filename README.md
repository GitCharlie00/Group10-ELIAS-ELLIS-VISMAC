# üë• Group10-ELIAS-ELLIS-VISMAC
### üìç Brunico (BZ) :calendar: 27-31/01/2025
#### *Adri√°n Szlatincs√°n*, *Claudio Schiavella*, *Dania Batool*, *Francesco Dibitonto*, *Francesco Pro*

## üìã Project Track
This project aims to develop a fine-tuning technique to mitigate gender and ethnicity biases in VLMs. The evaluation will focus on biases related to a predefined set of professions (e.g., doctor, nurse, etc‚Ä¶) using a provided evaluation dataset. Given that fine-tuning can influence the model‚Äôs generalization capabilities, students will also assess the zero-shot performance of the fine-tuned model on two domain-specific datasets. This dual evaluation will help identify trade-offs introduced by the debiasing process

### ‚úîÔ∏è Implemented Method
Due to the scarcity of datasets for mitigating race and gender biases, we decided to generate the first captions (prompts) and then corresponding images.
The prompts are all generated by specifying a race, a gender, and a corresponding job. 

The **gender** options are:
- **Male**
- **Female**
  
The option **races** are:
- **Asian**
- **Indian**
- **Black**
- **White**
  
**Job** categories are:
- **Driver**
- **Cleaner**
- **CEO**
- **Nurse**
- **Doctor**
- **Sheriff**
- **Chef**
- **Secretary**
  
Example of **captions** are:
- *A young female driver of indian descent who specializes in long-haul trucking known for his efficiency and excellent safety record*
- *A seasoned indian woman truck driver specializing in cross-country freight transport known for her expert navigation skills* 

With these prompts, we generated a total of 3521 images. 

These are two examples of generated images:

<img src="https://github.com/GitCharlie00/Group10-ELIAS-ELLIS-VISMAC/blob/main/imgs/prompt_58_00002_.png" alt="drawing" width="150"/>
<img src="https://github.com/GitCharlie00/Group10-ELIAS-ELLIS-VISMAC/blob/main/imgs/prompt_5_00001_.png" alt="drawing" width="150"/>

These images associated with the corresponding captions, were used to fine-tune [CLIP](https://github.com/openai/CLIP) model to mitigate its race and gender bias.
We evaluated the model bias though the following [Colab notebook](https://colab.research.google.com/drive/13tefNTV2AOAU6M182fvP7g8SYXX_IE3D?usp=sharing) provided by the winter school organizers.

Before our fine-tune, the model bias was the following:

<img src="https://github.com/GitCharlie00/Group10-ELIAS-ELLIS-VISMAC/blob/main/imgs/gender_bias.png" alt="drawing" width="600"/>
<img src="https://github.com/GitCharlie00/Group10-ELIAS-ELLIS-VISMAC/blob/main/imgs/race_bias.png" alt="drawing" width="600"/>


#### 



[Synthetic Demographic Dataset](https://www.kaggle.com/datasets/anthonytherrien/synthetic-population-demographics-dataset?resource=download)

[Dataset containing face images with gender and race but NOT professions](https://huggingface.co/datasets/HuggingFaceM4/FairFace)

[A model to generate images](https://huggingface.co/XLabs-AI/flux-RealismLora?)
